{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#\n",
        "Código 3.1: Avaliação da capacidade preditiva de um modelo"
      ],
      "metadata": {
        "id": "A2jKCAqG8Arm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Código 3.1\n",
        "#upload da Tabela3.1  https://github.com/TMLfour/ML/blob/main/Tabela3.1.xlsx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "tab31=pd.read_excel('/content/sample_data/Tabela3.1.xlsx')\n",
        "display(tab31.head(3))\n",
        "y=tab31['y']\n",
        "yhat=tab31['yhat']\n",
        "print(type(y))\n",
        "print(type(yhat))\n",
        "# SSE: Soma dos erros ao quadrado\n",
        "SSE = np.sum((y - yhat) ** 2)\n",
        "# MSE: Média dos erros ao quadrado\n",
        "MSE = mean_squared_error(y, yhat)\n",
        "# RMSE: Raiz quadrada do MSE\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(f\"SSE: {round(SSE, 2)}\")\n",
        "print(f\"MSE: {round(MSE, 2)}\")\n",
        "print(f\"RMSE: {round(RMSE, 2)}\")\n",
        "\n",
        "# Cálculo do MAE\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(y, yhat)\n",
        "print(f\"Erro Médio Absoluto (MAE): {round(mae, 2)}\")\n",
        "\n",
        "#Cálculo de EP\n",
        "ep=(y-yhat)/y*100\n",
        "print(f\"Erro Percentual (EP em %):\\n {round(ep, 2)}\")\n",
        "import matplotlib.pyplot as plt\n",
        "# Criando o diagrama de pontos sem linhas, mas com grade\n",
        "plt.figure(figsize=(6, 4))\n",
        "#Gráfico sem linhas e com grade\n",
        "plt.scatter(ep.index, ep, color='blue', s=100, edgecolor='black', alpha=0.7)\n",
        "plt.grid(True, linestyle=':', linewidth=1, color='gray')\n",
        "plt.axhline(y=0, color='black', linestyle='-', linewidth=2)\n",
        "# Configurações adicionais\n",
        "plt.title('Erros Percentuais')\n",
        "plt.xlabel('observação')\n",
        "plt.ylabel('EP%')\n",
        "plt.yticks(range(-15, 20, 5))  # Definindo o intervalo para o eixo Y\n",
        "plt.show()\n",
        "\n",
        "#Cálculo de MAPE\n",
        "mape = np.mean(np.abs((y - yhat) / y)) * 100 # necessário multiplicar por 100\n",
        "#apesar do nome MAPE do Python nao dá o valor em porcentagem\n",
        "print(f\"Erro Médio Absoluto Percentual (MAPE): {round(mape, 2)}%\")"
      ],
      "metadata": {
        "id": "vxM-I0ZviJ2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código 3.2 :  Avaliação de modelos de classificação binária"
      ],
      "metadata": {
        "id": "yhrCAFWqHlfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Código 3.2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc,accuracy_score, precision_score, recall_score,f1_score,matthews_corrcoef,confusion_matrix, log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "tab33=pd.read_excel('/content/sample_data/Tabela3.3.xlsx')\n",
        "display(tab33.head(3))\n",
        "\n",
        "# Avaliação do modelo utilizando pc=0,5\n",
        "pc=0.5\n",
        "y_class = (tab33['pr.pos'] >= pc).astype(int)\n",
        "accuracy = accuracy_score(tab33['status'], y_class)\n",
        "precision = precision_score(tab33['status'], y_class)\n",
        "recall = recall_score(tab33['status'], y_class)\n",
        "f1 = f1_score(tab33['status'], y_class)\n",
        "MCC=matthews_corrcoef(tab33['status'], y_class)\n",
        "conf_matrix = confusion_matrix(tab33['status'], y_class)\n",
        "conf_matrix=pd.DataFrame(conf_matrix)\n",
        "print(\"\\n\\nResultados\")\n",
        "print(f\"Acurácia: {accuracy:.3f}\")\n",
        "print(f\"Precisão: {precision:.3f}\")\n",
        "print(f\"Recall: {recall:.3f}\")\n",
        "tn, fp, fn, tp = confusion_matrix(tab33['status'], y_class).ravel()\n",
        "# Calculando a especificidade\n",
        "especificidade = tn / (tn + fp)\n",
        "print(f'Especificidade: {especificidade:.3f}')\n",
        "print(f\"F1-score: {f1:.3f}\")\n",
        "print(f\"Mathews correlation coeficient: {MCC:.3f}\")\n",
        "#matriz de classificação\n",
        "matriz = confusion_matrix(tab33['status'], y_class)\n",
        "conf_matrix = pd.DataFrame(matriz, index=['Real negativo', 'Real positivo'], columns=['Predito negativo', 'Predito positivo'])\n",
        "print(f\"\\nMatriz de classificação:\\n{conf_matrix.T}\")"
      ],
      "metadata": {
        "id": "DZnkBr-2aDiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código 3.3: Métricas baseadas em probabilidades"
      ],
      "metadata": {
        "id": "sq0ViMdxIFBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Código 3.3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc,accuracy_score, precision_score, recall_score,f1_score,matthews_corrcoef,confusion_matrix, log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "tab33=pd.read_excel('/content/sample_data/Tabela3.3.xlsx')\n",
        "display(tab33.head(3))\n",
        "#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "# Calcular a curva ROC\n",
        "fpr, tpr, thresholds = roc_curve(tab33[\"status\"], tab33['pr.pos'])\n",
        "# Calcular a AUC (Área sob a Curva)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plotar a curva ROC\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Taxa de Falsos Positivos (FPr)')\n",
        "plt.ylabel('Taxa de Verdadeiros Positivos (TPr)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "#cálculo de logloss\n",
        "true=(1,0,1,0,0) #classes reais\n",
        "pre1=(0.94,0.28,0.72,0.68,0.22) #previsão com modelo1\n",
        "pre2=(0.54,0.41,0.61,0.52,0.43) #previsão com modelo2\n",
        "pre3=(0.5,0.5,0.5,0.5,0.5) #previsão com modelo3\n",
        "logloss1 = log_loss(true, pre1)\n",
        "logloss2 = log_loss(true, pre2)\n",
        "logloss3 = log_loss(true, pre3)\n",
        "print(f'Log Loss1: {logloss1:.4f}')\n",
        "print(f'Log Loss2: {logloss2:.4f}')\n",
        "print(f'Log Loss3: {logloss3:.4f}')\n",
        "\n",
        "#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "# curvas para cálculo de KS  para a tabela 3.3\n",
        "# Separamos as probabilidades de acordo com o status (0 e 1)\n",
        "prob_0 = tab33[tab33['status'] == 0]['pr.pos']\n",
        "prob_1 = tab33[tab33['status'] == 1]['pr.pos']\n",
        "\n",
        "# Ordenar as probabilidades\n",
        "prob_0_sorted = np.sort(prob_0)\n",
        "prob_1_sorted = np.sort(prob_1)\n",
        "# Calcular a CDF (Função de Distribuição Acumulada)\n",
        "cdf_0 = np.arange(1, len(prob_0_sorted) + 1) / len(prob_0_sorted)\n",
        "cdf_1 = np.arange(1, len(prob_1_sorted) + 1) / len(prob_1_sorted)\n",
        "#definições auxiliares\n",
        "good = tab33[tab33['status'] == 0]\n",
        "bad = tab33[tab33['status'] == 1]\n",
        "cdf_bad = cdf_1\n",
        "# Calcular a distância entre as duas CDFs\n",
        "# Para comparar as CDFs nas mesmas probabilidades,\n",
        "#  interpolamos os valores de cdf_1 para as probabilidades de prob_0_sorted\n",
        "cdf_bad_interp = np.interp(good['pr.pos'], bad['pr.pos'], cdf_bad)\n",
        "# Calculando as distâncias\n",
        "distances = np.abs(cdf_0 - cdf_bad_interp)\n",
        "# Encontrando a maior distância\n",
        "max_distance = np.max(distances)\n",
        "max_distance_index = np.argmax(distances)\n",
        "max_distance_prob = prob_0_sorted[max_distance_index]\n",
        "# Plotando o gráfico\n",
        "plt.figure(figsize=(6, 4))\n",
        "# CDF para o status 0\n",
        "plt.plot(prob_0_sorted, cdf_0, label='negativo', color='blue',linestyle='--', lw=2)\n",
        "# CDF para o status 1\n",
        "plt.plot(prob_1_sorted, cdf_1, label='positivo', color='red', lw=2)\n",
        "# Destacar a maior distância\n",
        "plt.scatter(max_distance_prob, cdf_0[max_distance_index], color='black', zorder=5)\n",
        "plt.scatter(max_distance_prob, cdf_bad_interp[max_distance_index], color='black', zorder=5)\n",
        "plt.text(max_distance_prob, (cdf_0[max_distance_index] + cdf_bad_interp[max_distance_index]) / 2,\n",
        "         f'KS: {max_distance:.2f}', color='black', ha='center')\n",
        "# Configurações do gráfico\n",
        "plt.xlabel('pr.pos')\n",
        "plt.ylabel('CDFn e CDFp')\n",
        "plt.title('Distribuições Acumuladas de Probabilidade')\n",
        "plt.legend()\n",
        "# Adicionar grid pontilhado\n",
        "plt.grid(True, linestyle=':', color='gray')\n",
        "# Exibir o gráfico\n",
        "plt.show()\n",
        "# Imprimir a maior distância e o ponto\n",
        "print(\"KS para resultados com reglog\")\n",
        "print(f'KS: {max_distance:.3f} na probabilidade {max_distance_prob:.3f}')\n",
        "\n",
        "#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "#Cálculo da cross entropy (tabela 3.11)\n",
        "#upload https://github.com/TMLfour/ML/blob/main/Tabela3.11.xlsx\n",
        "tab311=pd.read_excel('/content/sample_data/Tabela3.11.xlsx')\n",
        "display(tab311.head(3))\n",
        "\n",
        "y1 = np.array(tab311[\"y1\"])\n",
        "p1 = np.array(tab311[\"p1\"])\n",
        "y2 = np.array(tab311[\"y2\"])\n",
        "p2 = np.array(tab311[\"p2\"])\n",
        "y3 = np.array(tab311[\"y3\"])\n",
        "p3= np.array(tab311[\"p3\"])\n",
        "# Define uma função para calcular a cross-entropy média\n",
        "def cross_entropy(y_true, y_pred):\n",
        "      ce = - (y_true * np.log(y_pred))\n",
        "      return np.sum(ce)\n",
        "# Calcula para cada amostra\n",
        "ce1 = cross_entropy(y1, p1)\n",
        "ce2 = cross_entropy(y2, p2)\n",
        "ce3 = cross_entropy(y3, p3)\n",
        "CE=np.sum([ce1,ce2,ce3])/10\n",
        "print(f\"CE=: {round(CE, 3)}\")"
      ],
      "metadata": {
        "id": "n-mloqshH064"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código 3.4:  Resampling"
      ],
      "metadata": {
        "id": "2FkJ-EEWJGvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Código 3.4\n",
        "#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "### cross validation\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "res=pd.read_excel('/content/sample_data/resamp.xlsx') # ajuste o caminho se necessário\n",
        "display(res.head(3))\n",
        "X=np.array(res[\"x\"]).reshape(-1, 1) #sklearn pede 2D\n",
        "y=res[\"y\"]\n",
        "\n",
        "# Modelo de regressão\n",
        "model = LinearRegression()\n",
        "\n",
        "# K-Fold Cross Validation\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=18)\n",
        "# Armazenar MAPE\n",
        "mape_scores = []\n",
        "\n",
        "# Loop com índice, para identificae sequencia do CV\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X), start=1):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    percentage_error = np.abs((y_test - y_pred) / y_test) * 100\n",
        "    #MAPE do Python nao está em %\n",
        "    mape_fold = np.mean(percentage_error)\n",
        "    mape_scores.append(mape_fold)\n",
        "    print(f\"Fold {i}: MAPE = {mape_fold:.3f}\")\n",
        "# MAPE médio ao final\n",
        "print(f\"\\nMAPE médio: {np.mean(mape_scores):.3f}\")\n",
        "print(f\"Desvio padrão do MAPE:  {np.std(mape_scores):.3f}\")\n",
        "\n",
        "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "### Multiple Cross Validation\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import pandas as pd\n",
        "\n",
        "res=pd.read_excel('/content/sample_data/resamp.xlsx') # ajuste o caminho se necessário\n",
        "display(res.head(3))\n",
        "X=np.array(res[\"x\"]).reshape(-1, 1)\n",
        "y=res[\"y\"]\n",
        "\n",
        "# Criar RepeatedKFold (5 folds, repetido 10 vezes)\n",
        "rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=18)\n",
        "\n",
        "# Modelo\n",
        "model = LinearRegression()\n",
        "\n",
        "# Armazenar scores\n",
        "scores = []\n",
        "\n",
        "for train_index, test_index in rkf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    mape = mean_absolute_percentage_error(y_test, preds)*100 # esta função precisa *100 para dar em %\n",
        "    scores.append(mape)\n",
        "\n",
        "print(f\"Número total de execuções: {len(scores)}\")\n",
        "print(f\"MAPE médio: {np.mean(scores):.3f}\")\n",
        "print(f\"Desvio padrão do MAPE:  {np.std(scores):.3f}\")\n",
        "\n",
        "#&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
        "# Bootstrap sampling\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "#vou utilizar outa forma de regressão para ilustração.\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.utils import resample\n",
        "\n",
        "res=pd.read_excel('/content/sample_data/resamp.xlsx')# ajuste o caminho se necessário\n",
        "\n",
        "# Parâmetros\n",
        "n_iterations = 50\n",
        "results = []\n",
        "np.random.seed(9)\n",
        "for i in range(n_iterations):\n",
        "    # Bootstrap amostra\n",
        "    bootstrap_sample = resample(res, replace=True, n_samples=len(res))\n",
        "\n",
        "    # OOB: dados que não foram selecionados\n",
        "    oob_indices = list(set(res.index) - set(bootstrap_sample.index))\n",
        "    oob_sample = res.loc[oob_indices]\n",
        "\n",
        "    # Ajustar o modelo\n",
        "    x_boot = sm.add_constant(bootstrap_sample['x'])\n",
        "    model = sm.OLS(bootstrap_sample['y'], x_boot).fit()\n",
        "\n",
        "    # Avaliação com dados OOB\n",
        "    if not oob_sample.empty:\n",
        "        x_oob = sm.add_constant(oob_sample['x'])\n",
        "        y_oob_pred = model.predict(x_oob)\n",
        "        mape_oob = mean_absolute_percentage_error(oob_sample['y'], y_oob_pred)*100\n",
        "    else:\n",
        "        mape_oob = np.nan  # No OOB samples (rare with small datasets)\n",
        "    # Armazenar resultados\n",
        "    results.append({\n",
        "        'intercept': model.params['const'],\n",
        "        'slope': model.params['x'],\n",
        "        'mape_oob': mape_oob\n",
        "    })\n",
        "# Converter resultados para DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "# Exibir estatísticas resumidas\n",
        "print(\"\\n\\n Resultados do bootsrap sampling\")\n",
        "print(results_df.describe())\n",
        "\n",
        "beta=results_df['slope'].mean()# média dos coeficientes\n",
        "alfa=results_df['intercept'].mean()# média dos interceptos\n",
        "MAPE_OOB=results_df['mape_oob'].mean()# erro médio OOB\n",
        "MAPE_OOB_SD=results_df['mape_oob'].std()\n",
        "\n",
        "print(f\"INTERCEPTO=Média dos interceptos: {alfa:.3f}\")\n",
        "print(f\"COEF=Média dos coeficientes: {beta:.3f}\")\n",
        "print(f\"MAPE médio baseado nos casos OOB: {mape_oob:.3f}%\")\n",
        "print(f\"Desvio padrão dos MAPE baseados nos casos OOB: {MAPE_OOB_SD:.3f}%\")"
      ],
      "metadata": {
        "id": "o7GC4J49_ncD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}